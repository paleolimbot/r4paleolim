[
["index.html", "R for Paleolimnology Introduction 0.1 Prerequisites 0.2 Other places to learn R/RStudio/Tidyverse 0.3 Colophon", " R for Paleolimnology Brent Thorne and Dewey Dunnington 2018-05-01 Introduction TODO: write this section 0.1 Prerequisites R, RStudio, tidyverse TODO: installing R and RStudio, turning off autoload/save of workspace. 0.2 Other places to learn R/RStudio/Tidyverse Grolemund and Wickham (2017) (referenced many times in these tutorials) The Introduction to the tidyverse Data Camp course by David Robinson. 0.3 Colophon This course material was written using the bookdown package inside RStudio. Pages were built using Travis CI, pandoc and gitbook. The source is available on github. References "],
["basic-r.html", "Tutorial 1 Basic R 1.1 Prerequisites 1.2 Expressions and Variables 1.3 Functions 1.4 Vectors 1.5 Indexing 1.6 Missing Values 1.7 Data Frames 1.8 Loading Packages 1.9 Using the Script Editor 1.10 The Environment 1.11 Excercises 1.12 Summary", " Tutorial 1 Basic R When you open RStudio, you are presented with the ominous &gt; of the R console. By the end of this tutorial, hopefully that &gt; should fill you with a feeling of hope and opportunity, for magical things can happen when you type the right thing after the &gt;. This short introduction will get you started with the R console so that when we introduce more powerful functions, you can understand what R is doing at the base level! The tutorial is loosely based on the Workflow: basics tutorial in the free online book, R for Data Science. 1.1 Prerequisites To complete this tutorial, you will need R and RStudio installed, and RStudio open. If you can see the &gt; of the R console in the lower left part of your screen, you are good to go! The basic R console. 1.2 Expressions and Variables Let’s start with the basics. Try typing in something like this at the prompt: 1 + 1 ## [1] 2 2 * 5 ## [1] 10 5 ^ 2 ## [1] 25 2 * (5 + 1) ## [1] 12 As you can see, R works just like a calculator and evaluates all of these expressions just like you would expect. If we would like to save the result of one of these expressions we can assign that value to a variable like this: x &lt;- 1 + 1 Then, to view the value of x we can just type x at the console, and R will show us the value. x ## [1] 2 The &lt;- means “assign the value on the right to the variable on the left”. We can also use x in any expression and R will substitute its value in like this: x + 2 ## [1] 4 An expression is something that R can evaluate to produce a value, like 2+2 or x + 2. Any time you type this in the console without assigning it to a variable, R will print out the value. In fact, any time you type anything into the R console, R is evaluating that expression, which may or may not return a value. If there is no value returned, R won’t print anything when you press enter. So far we’ve just used numbers, but often we need to enter text into R. Whenever we do this, we surround the text in quotes, like this: mytext &lt;- &quot;I am text&quot; Text (called character vectors) are one of many data types available in R. 1.3 Functions A function is some kind of operation that takes one or more arguments (input values) and produces a return value (output value). The sqrt() function is a good example: sqrt(4) ## [1] 2 Here, 4 is an argument, and the function returns the square root of that, which is 2. Functions can take more arguments, like the max function: max(2, 6, 7, 2, 10) ## [1] 10 Here we are giving the max function 5 arguments, of which it returns the maximum. One other way we specify arguments is by keyword arguments, like in the paste function. paste(&quot;string1&quot;, &quot;string2&quot;, sep = &quot;_&quot;) ## [1] &quot;string1_string2&quot; The paste function takes its arguments and combines them using the sep that we specify, in this case &quot;_&quot;. The function returns this string. Many functions in R have many many arguments and usually we only want to modify one or two of interest to us. The format is always key=value, where key is the name of the keyword and value is some expression we would like to use as the value for that argument. R contains thousands of functions that do most of what you could possibly imagine with regards to data and statistics, but remembering which one you want and how to use it can be difficult. Luckily, RStudio makes it easy using tab autocompletion and easy access to help files. To autocomplete, start typing the name of the function and press the [Tab] key (or Ctrl+Space), and RStudio will helpfully provide you with suggestions. To access the documentation for a particular function, you can type ? in front of the function name and press return, and RStudio will open the help file for you if it exists. ?paste Will bring up the following: 1.3 Description Concatenate vectors after converting to character. 1.3 Usage paste (..., sep = &quot; &quot;, collapse = NULL) paste0(..., collapse = NULL) 1.3 Arguments … one or more R objects, to be converted to character vectors. sep a character string to separate the terms. Not NA_character_. collapse an optional character string to separate the results. Not NA_character_. Note that each of these arguments can be specified by keyword, and have default values that we can see in the usage section. The ... means that we can pass any number of arguments to the function. There’s too many functions in R to keep in your head at one time, so getting good at reading these help files is very useful! 1.4 Vectors So far we’ve just been dealing with single values like 2+2 or &quot;mystring&quot;, but the real power of R is that it can operate easily on large lists of data, so it makes sense that it provides us with an easy way to work with this data. These lists of data are called vectors, and we create them using the c function (c stands for concatenate, or join together). myvector &lt;- c(10, 9, 8, 7, 2) myvector ## [1] 10 9 8 7 2 Here the c function took our arguments of 10, 9, 8, 7, and 2, and returned a vector, which we assigned to the variable named myvector. When we evaluated myvector, it printed out the list of values it contained. Vectors don’t just have to contain numbers, they can contain strings as well. mytextvector &lt;- c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;) mytextvector ## [1] &quot;word1&quot; &quot;word2&quot; &quot;word3&quot; Here the c function took our arguments and returned a vector of strings, which we assigned to the variable mytextvector. It is common to have to generate a vector of all the integer values between two numbers, so R provides a short form for this: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 20:12 ## [1] 20 19 18 17 16 15 14 13 12 myothervector &lt;- 13:16 myothervector ## [1] 13 14 15 16 You can also use an expression on either side of the :, like this: (5^2):(3*10) ## [1] 25 26 27 28 29 30 start &lt;- 25 end &lt;- start + 5 start:end ## [1] 25 26 27 28 29 30 1.5 Indexing Now we’ve created vectors, but to get at what’s inside them we need to retrieve values using an index. We do this using square brackets like this: myvector &lt;- c(10, 9, 8, 7, 2) myvector[1] ## [1] 10 myvector[5] ## [1] 2 myvector[3] ## [1] 8 This code creates a vector, assigns it to the variable myvector, then retrieves the 1st, 5th, and 3rd value stored in that vector. If we would like multiple values from the vector, we can pass multiple values as indices, like this: myvector[c(1, 5, 3)] ## [1] 10 2 8 You’ll notice that the index that we’re using is actually a vector itself! I know, we’re using a vector to index a vector and it’s a little trippy, but it’s incredibly useful. You’ll remember that we can easily create vectors of sequential integers, which we can use to get a sequence of values from a vector by using it as an index. myvector[1:3] ## [1] 10 9 8 This would be equivalent to: myvector[c(1, 2, 3)] ## [1] 10 9 8 There is one other useful way to index a vector using a vector, which is to use a TRUE/FALSE vector. This is probably the most useful of the indexing methods, because it allows you to do things like: myvector[myvector &gt; 7] ## [1] 10 9 8 This works because myvector &gt; 7 is, itself, a TRUE/FALSE vector with the same length as myvector, indicating whether or not it is greater than 7 at any given position. myvector &gt; 7 ## [1] TRUE TRUE TRUE FALSE FALSE 1.6 Missing Values Missing values are represented in R using NA, or “not assigned”. The fact that R handles missing values is one of the best reasons to use R for data analysis, because missing values are common in real data. NA values are propogated by R functions, meaning that if you take the mean() of a vector containing a missing value, it will give you NA as the average! mean(c(NA, 1, 2, 3)) ## [1] NA This is rarely what you want, but is a good practice to explicitly handle missing values in your code, and R forces you to do this. To avoid getting an NA back, you can use na.rm = TRUE, an argument that is available in many R functions. mean(c(NA, 1, 2, 3), na.rm = TRUE) ## [1] 2 1.7 Data Frames The vast majority of data in R is kept in a data frame, which is a collection of vectors of the same length. You can think of a data frame as a table, with each column in the table being of the same type (numeric, character, TRUE/FALSE, etc.). In the following tutorials we will use a modern version of the data frame called a tibble, which behaves more predictably than base R’s version. For now, we will use base R’s function to create a data frame, data.frame(). my_data_frame &lt;- data.frame(number = c(1, 2, 3), name = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), is_one = c(TRUE, FALSE, FALSE)) my_data_frame ## # A tibble: 3 x 3 ## number name is_one ## &lt;dbl&gt; &lt;fct&gt; &lt;lgl&gt; ## 1 1.00 one T ## 2 2.00 two F ## 3 3.00 three F The syntax for creating a data frame is data.frame(column_name = value). The above data frame has three columns (number, a numeric column, name, a character column, and is_one, a logical TRUE/FALSE column). You can get these values as vectors again using the $ operator, which allows you to extract a vector from a data frame. my_data_frame$number ## [1] 1 2 3 my_data_frame$name ## [1] one two three ## Levels: one three two (Whoa! What does “Levels” mean!? Don’t worry about this for now…it is one of the reasons we will be using the tibble() function to create data frames in the following tutorials.) my_data_frame$is_one ## [1] TRUE FALSE FALSE You can think of data frames a collection of vectors (variables) having the same number of elements. The number of elements has to be equal for all columns, meaning the ith value of each vector forms an observation. Data in this form is very useful and is the subject of most of the following tutorials. 1.8 Loading Packages Basic R functionality is designed to provide basic functions to help with data analysis, but may add-ons are available and code you find online (including here, shortly) will often tell you to load a “package” using library(). This will be a call to the library() function in the form library(packagename), where packagename is the name of the package which contains the functions you are interested in using (all of the subsequent tutorials will use library(tidyverse), because the tidyverse package contains many useful functions that you will use on a regular basis. When you call library(packagename), it will make these functions available to you. Occasionally you will see something like packagename::function_name(), which is a method to use a function without making all of the functions in a package. This is equivalent to typing library(packagename) then function_name() on the next line. You can install packages using install.packages(&quot;packagename&quot;) (note the quotes around packagename!). For example, let’s install the tidyverse package now, since you’ll be using it in the rest of this series. install.pacakges(&quot;tidyverse&quot;) Then, load it using library(): library(tidyverse) ## ── Attaching packages ──────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.7.2 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.2.0 ## ── Conflicts ─────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Finally, we are ready to call a function from the package. The tidyverse package actually installs and loads a family of useful packages for us, a list of which we can access using tidyverse_packages(). Try it! tidyverse_packages() ## [1] &quot;broom&quot; &quot;cli&quot; &quot;crayon&quot; &quot;dplyr&quot; &quot;dbplyr&quot; ## [6] &quot;forcats&quot; &quot;ggplot2&quot; &quot;haven&quot; &quot;hms&quot; &quot;httr&quot; ## [11] &quot;jsonlite&quot; &quot;lubridate&quot; &quot;magrittr&quot; &quot;modelr&quot; &quot;purrr&quot; ## [16] &quot;readr&quot; &quot;readxl\\n(&gt;=&quot; &quot;reprex&quot; &quot;rlang&quot; &quot;rstudioapi&quot; ## [21] &quot;rvest&quot; &quot;stringr&quot; &quot;tibble&quot; &quot;tidyr&quot; &quot;xml2&quot; ## [26] &quot;tidyverse&quot; 1.9 Using the Script Editor In reality, very little of the code you type will be directly in the prompt. Instead, you will use RStudio’s script editor to run commands so that you can go back and edit them or run them from the beginning. To create a new R script, choose File, New File, and R script (you can also choose the little green “+” button at the top left of the console window). A blank R script should appear in a new tab. A new R script in the RStudio script editor. When you type a command in the editor and press enter, nothing happens! This is because the editor is meant to build script that contain multiple lines, unlike the console, which is meant to execute a single line at a time. To run a command you have typed in the script editor, press Ctrl+Enter (Command+Enter on a Mac). You can even select multiple lines, press Ctrl+Enter, and they will all run at once! You can also save the script and choose Source to run the whole thing. It is good practice to keep all of your code in a script somewhere. Typing it on a console repeatedly is hard work, and leads to errors! 1.10 The Environment When you’ve assigned a bunch of variables, it can be tricky to keep track of which ones are where and contain what! Hopefully you have given them short but descriptive names, but if you happen to forget you can check the “Environment” tab in RStudio (in the upper right part of the window). If you’d like to start fresh you can clear the environment (use the little broom icon or go to Session/Clear Workspace), and if you really want to start fresh, you can restart R using Session/Restart R. This will clear your workspace and unload all the packages you loaded using library(). This is a good way to make sure all of your analysis has been encapsulated by the script, since you can Restart R and Source your script to replicate your work. The RStudio Environment Tab By default, R will save your session when you quit and reload the variables you had previously assigned when you reopen it. This is dangerous, because even though you created an object, you may not be able to create it again! I highly reccomend using the script editor to encapsulate all of your code, and disable the automatic loading/saving of your workspace. You can do this in RStuio’s Preferences (or “Global Options” on Windows/Linux) by setting “Save workspace on exit” to “Never”, and unchecking “Restore .RData into workspace at startup”. The RStudio Global Options/Preferences window. 1.11 Excercises To practice the basics of R, complete the very first swirl module, R Programming / Basic Building Blocks. To do this, you’ll need to install and load swirl by typing this at the R prompt: install.packages(&quot;swirl&quot;) library(swirl) swirl() You should get a friendly greeting that will prompt you to choose a course (you want number 1, “R Programming: The basics of programming in R”) and a module (you want number 1, “Basic Building Blocks”). 1.12 Summary In this lesson we covered expressions, variables, functions, vectors, and indexing, all of which will help you get the most out of the tutorials in this series. For more information, check out the Workflow: basics and Workflow: scripts tutorial in the free online book, R for Data Science. "],
["working-with-tables-using-the-tidyverse.html", "Tutorial 2 Working with Tables using the Tidyverse 2.1 Prerequisites 2.2 Viewing a Data Frame 2.3 Selecting Columns 2.4 Filtering Rows 2.5 Selecting and Filtering 2.6 The Pipe (%&gt;%) 2.7 Sorting A Data Frame 2.8 Distinct Values 2.9 Summarising A Data Frame 2.10 Extracting Columns 2.11 Base R Subsetting vs. select() and filter() 2.12 Summary", " Tutorial 2 Working with Tables using the Tidyverse In this tutorial we will introduce the data frame, or the object type that R uses to store tables. Most of the data you will work with in R can be represented by a table (think an excel sheet), and one of the main advantages of using R is that the data frame is a powerful and intuitive interface for tabular data. In this tutorial we will use the tidyverse to manipulate and summarise tabular data. The tutorial is a companion to the Data transformation chapter in R for Data Science. 2.1 Prerequisites The prerequisite for this tutorial is the tidyverse package. If this package isn’t installed, you’ll have to install it using install.packages(). install.packages(&quot;tidyverse&quot;) Load the packages when you’re done! If there are errors, you may have not installed the above packages correctly! library(tidyverse) Finally, you will need to obtain the example data. Copy/paste the statement below to load the sample data for this tutorial (you can also download the dataset as a csv). valley_climate &lt;- read_csv(&quot;http://paleolimbot.github.io/r4wrs/data/valley_climate.csv&quot;, col_types = cols(.default = col_guess())) It’s worth mentioning a little bit about what this data frame contains, since we’ll be working with it for the rest of this tutorial. Each row contains a number of parameters that are available on a monthly basis from two Environment Canada climate stations (Kentville Agricultural Research Station and Greenwood Station, both located in the Annapolis Valley of Nova Scotia). The station_name, year, and month columns identify where and when the values were measured, and the rest of the columns contain the measured values. The only column names that are slightly cryptic are extr_max_temp and extr_min_temp, which are the extreme maximum and minimum temperatures measured in that month, respectively. 2.2 Viewing a Data Frame The variable we have just created (valley_climate) is a data frame, which is a table of values much like you would find in a spreadsheet. Each column in the data frame represents a variable (in this case, the year and month where each observation was measured, the mean temperature, total precipitation, and many others), and each row in the table represents an observation (for example, the mean temperature and total precipitation in May of 2000 at Greenwood). In RStudio’s “Environment” tab (usually at the top right of the screen), you should see a variable called valley_climate in the list. You can inspect it by clicking on the variable name, after which a tab will appear displaying the contents of the variable you just loaded. Clicking the little arrow to the left of the name will display the structure of the data frame, including the column names and some sample values. You can also do both of these things using the R commands View() and str(), respectively. Also useful is the head() function, which will display the first few rows of a data frame. View(valley_climate) # will display a graphic table browser str(valley_climate) # will display a text summary of the object ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 96 obs. of 11 variables: ## $ station_name : chr &quot;Kentville Cda Cs&quot; &quot;Kentville Cda Cs&quot; &quot;Kentville Cda Cs&quot; &quot;Kentville Cda Cs&quot; ... ## $ province : chr &quot;Nova Scotia&quot; &quot;Nova Scotia&quot; &quot;Nova Scotia&quot; &quot;Nova Scotia&quot; ... ## $ year : int 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ... ## $ month : int 1 2 3 4 5 6 7 8 9 10 ... ## $ date : Date, format: &quot;2000-01-01&quot; &quot;2000-02-01&quot; ... ## $ mean_max_temp: num 0.5 2.2 5.8 10.4 15.1 22.4 24.3 24.4 19.7 14.8 ... ## $ mean_min_temp: num -8.9 -7.1 -2.6 2.3 4.9 10.3 13.2 13.9 9.4 4.9 ... ## $ mean_temp : num -4.2 -2.6 1.6 6.4 10 16.4 18.8 19.2 14.6 9.9 ... ## $ extr_max_temp: num 17.3 16.9 18.2 19.8 21.1 32.6 28.2 28.8 29.9 21.8 ... ## $ extr_min_temp: num -21.9 -18.2 -10.3 -4.9 -0.4 0.7 8.5 7.3 -0.8 -2.3 ... ## $ total_precip : num 182.2 10.7 116.4 89.1 58 ... head(valley_climate) ## # A tibble: 6 x 11 ## station_name province year month date mean_max_temp mean_min_temp ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Kentville C… Nova Sc… 2000 1 2000-01-01 0.500 - 8.90 ## 2 Kentville C… Nova Sc… 2000 2 2000-02-01 2.20 - 7.10 ## 3 Kentville C… Nova Sc… 2000 3 2000-03-01 5.80 - 2.60 ## 4 Kentville C… Nova Sc… 2000 4 2000-04-01 10.4 2.30 ## 5 Kentville C… Nova Sc… 2000 5 2000-05-01 15.1 4.90 ## 6 Kentville C… Nova Sc… 2000 6 2000-06-01 22.4 10.3 ## # ... with 4 more variables: mean_temp &lt;dbl&gt;, extr_max_temp &lt;dbl&gt;, ## # extr_min_temp &lt;dbl&gt;, total_precip &lt;dbl&gt; 2.3 Selecting Columns One way to subset valley_climate is to subset by column, for which we will use the select() function. For example, we may only be interested in the mean temperature information, represented by the columns station_name, year, month, and mean_temp. mean_temp_data &lt;- select(valley_climate, station_name, year, month, mean_temp) The first argument to the select() function is the original data frame (in this case, valley_climate), and the remaining arguments are the names of the columns to be selected. To select the station_name, year, month, mean_temp and total_precip columns, you would use the following R command: temp_precip_data &lt;- select(valley_climate, station_name, year, month, mean_temp, total_precip) 2.3.1 Excercises Use View(), str(), and head() to preview the two data frames we just created. Do they have the columns you would expect? Use select() to select station_name, year, month, and all of the columns containing temperature values, and assign it to the variable temp_data. 2.4 Filtering Rows Another way to subset valley_climate is by filtering rows using column values, similar to the filter feature in Microsoft Excel. This is done using the filter() function. For example, we may only be interested in July temperature for the two stations. july_data &lt;- filter(valley_climate, month == 7) Just like select(), the first argument to filter() is the original data frame, and the subsequent arguments are the conditions that each row must satisfy in order to be included in the output. Column values are referred to by the column name (in the above example, month), so to include all rows where the value in the month column is 7, we use month == 7. Passing multiple conditions means each row must satisfy all of the conditions, such that to obtain the data for July of 2000, we can use the following call to filter(): july_2000_data &lt;- filter(valley_climate, month == 7, year == 2000) It is very important that there are two equals signs within filter()! The == operator tests for equality (e.g. (2 + 2) == 4), whereas the = operator assigns a value or passes a named argument to a function, which is not what you’re trying to do within filter(). Other common operators that are useful within filter are != (not equal to), &gt; (greater than), &lt; (less than), &gt;= (greater than or equal to), &lt;= (less than or equal to), and %in% (tests if the value is one of several values). Using these, we could find out which observations had mean temperatures that were below freezing: freezing_observations &lt;- filter(valley_climate, mean_temp &lt; 0) We could also find which observations occurred during the summer months (May, June, July, or August): summer_data &lt;- filter(valley_climate, month %in% c(5, 6, 7, 8)) 2.4.1 Exercises Use View(), str(), and head() to preview the data frames we just created. Do they have the rows you would expect? Use filter() to find observations from the month of December where the mean temperature was above freezing. Are there any observations from the month of January where the mean temperature was below freezing? Filter valley_climate to include only observations from the months of December, January, February, and March and assign it to a variable name of your choosing. 2.5 Selecting and Filtering Often we need to use both select() and filter() to obtain the desired subset of a data frame. To do this, we need to pass the result of select() to filter(), or the result of filter() to select. For example, we could create a data frame of mean temperature observations from the month of July one of two ways (you’ll recall that we selected temperature columns in the data frame mean_temp_data, and we filtered for the month of July in the data frame july_data): july_temp &lt;- filter(mean_temp_data, month == 7) july_temp2 &lt;- select(july_data, station_name, year, month, mean_temp) 2.5.1 Exercises Inspect july_temp and july_temp2 using View(), str(), and head(). Are they identical? Create a data frame of July total precipitation data and give it a variable name of your choosing. Do this by using select() followed by filter(), then using filter() followed by select(). Inspect the output to ensure the data frames are identical. 2.6 The Pipe (%&gt;%) There is an easier way! Instead of creating intermediary variables every time we want to subset a data frame using select() and filter(), we can use the pipe operator (%&gt;%) to pass the result of one function call to another. Thus, creating our july_temp data frame from above becomes one line with one variable assignment instead of two. july_temp3 &lt;- valley_climate %&gt;% filter(month == 7) %&gt;% select(station_name, year, month, mean_temp) What %&gt;% does is pass the left side into the first argument of the function call on the right side. Thus, filter(valley_climate, month == 7) becomes valley_climate %&gt;% filter(month ==7). When using the tidyverse family of packages, you should use the pipe as often as possible! It usually makes for more readable, less error-prone code, and reduces the number of temporary variables you create that clutter up your workspace. When using filter() and select() with other tidyverse manipulations like arrange(), group_by(), summarise(), and mutate(), the pipe becomes indispensable. 2.6.1 Exercises Inspect july_temp3 to ensure it is identical to july_temp. Create a data frame of July total precipitation data using valley_climate, filter(), select(), and %&gt;%. Is it identical to the data frame you created in the exercise above? 2.7 Sorting A Data Frame Sometimes it is desirable to view rows in a particular order, which can be used to quickly determine min and max values of various parameters. You can do this in the interactive editor using View(), but sometimes rows need to be in particular order for plotting or other analysis. This is done using the arrange() function. For example, it may make sense to view valley_climate in ascending year and month order (most recent last): valley_climate %&gt;% arrange(station_name, year, month) ## # A tibble: 96 x 11 ## station_name province year month date mean_max_temp ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Greenwood A Nova Scotia 2000 1 2000-01-01 1.20 ## 2 Greenwood A Nova Scotia 2000 2 2000-02-01 2.90 ## 3 Greenwood A Nova Scotia 2000 3 2000-03-01 7.10 ## 4 Greenwood A Nova Scotia 2000 4 2000-04-01 11.2 ## 5 Greenwood A Nova Scotia 2000 5 2000-05-01 16.0 ## 6 Greenwood A Nova Scotia 2000 6 2000-06-01 22.7 ## 7 Greenwood A Nova Scotia 2000 7 2000-07-01 25.0 ## 8 Greenwood A Nova Scotia 2000 8 2000-08-01 24.7 ## 9 Greenwood A Nova Scotia 2000 9 2000-09-01 20.5 ## 10 Greenwood A Nova Scotia 2000 10 2000-10-01 14.9 ## # ... with 86 more rows, and 5 more variables: mean_min_temp &lt;dbl&gt;, ## # mean_temp &lt;dbl&gt;, extr_max_temp &lt;dbl&gt;, extr_min_temp &lt;dbl&gt;, ## # total_precip &lt;dbl&gt; Or descending year and month order (most recent first): valley_climate %&gt;% arrange(station_name, desc(year), desc(month)) ## # A tibble: 96 x 11 ## station_name province year month date mean_max_temp ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Greenwood A Nova Scotia 2003 12 2003-12-01 3.60 ## 2 Greenwood A Nova Scotia 2003 11 2003-11-01 9.70 ## 3 Greenwood A Nova Scotia 2003 10 2003-10-01 15.4 ## 4 Greenwood A Nova Scotia 2003 9 2003-09-01 23.1 ## 5 Greenwood A Nova Scotia 2003 8 2003-08-01 24.6 ## 6 Greenwood A Nova Scotia 2003 7 2003-07-01 26.9 ## 7 Greenwood A Nova Scotia 2003 6 2003-06-01 23.2 ## 8 Greenwood A Nova Scotia 2003 5 2003-05-01 16.2 ## 9 Greenwood A Nova Scotia 2003 4 2003-04-01 9.30 ## 10 Greenwood A Nova Scotia 2003 3 2003-03-01 4.50 ## # ... with 86 more rows, and 5 more variables: mean_min_temp &lt;dbl&gt;, ## # mean_temp &lt;dbl&gt;, extr_max_temp &lt;dbl&gt;, extr_min_temp &lt;dbl&gt;, ## # total_precip &lt;dbl&gt; The arrange() function takes columns as arguments, surrounded by desc() if that column should be sorted in descending order. 2.8 Distinct Values It is often useful to know which values exist in a data frame. For example, I’ve told you that the two locations are for Kentville and Greenwood, but what are they actually called in the dataset? To do this, we can use the distinct() function. valley_climate %&gt;% distinct(station_name) ## # A tibble: 2 x 1 ## station_name ## &lt;chr&gt; ## 1 Kentville Cda Cs ## 2 Greenwood A The distinct() function can also take multiple columns, which will reduce the data frame to the unique combinations of those variables. For example, we could find out which years are represented in the data for each station. valley_climate %&gt;% distinct(station_name, year) ## # A tibble: 8 x 2 ## station_name year ## &lt;chr&gt; &lt;int&gt; ## 1 Kentville Cda Cs 2000 ## 2 Kentville Cda Cs 2001 ## 3 Kentville Cda Cs 2002 ## 4 Kentville Cda Cs 2003 ## 5 Greenwood A 2000 ## 6 Greenwood A 2001 ## 7 Greenwood A 2002 ## 8 Greenwood A 2003 The distinct() function can take any number of column names as arguments. 2.9 Summarising A Data Frame So far we have looked at subsets of valley_climate, but what if we want yearly averages instead of monthly averages? Using the tidyverse, we can group_by() the station_name and year column, and summarise(): valley_climate %&gt;% group_by(station_name, year) %&gt;% summarise(mean_temp = mean(mean_temp)) ## # A tibble: 8 x 3 ## # Groups: station_name [?] ## station_name year mean_temp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Greenwood A 2000 7.54 ## 2 Greenwood A 2001 7.72 ## 3 Greenwood A 2002 7.46 ## 4 Greenwood A 2003 7.22 ## 5 Kentville Cda Cs 2000 7.62 ## 6 Kentville Cda Cs 2001 8.01 ## 7 Kentville Cda Cs 2002 NA ## 8 Kentville Cda Cs 2003 7.32 Here group_by() gets a list of columns, for which each unique combination of values will get one row in the output. summarise() gets a list of expressions that are evaluated for every unique combination of values defined by group_by() (e.g., mean_temp is the mean() of the mean_temp column for each station, for each year). Often, we want to include a number of summary columns in the output, which we can do by pasing more expressions to summarise(): valley_climate %&gt;% group_by(station_name, year) %&gt;% summarise(mean_temp = mean(mean_temp), max_temp = max(extr_max_temp), min_temp = min(extr_min_temp)) ## # A tibble: 8 x 5 ## # Groups: station_name [?] ## station_name year mean_temp max_temp min_temp ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Greenwood A 2000 7.54 31.8 -22.3 ## 2 Greenwood A 2001 7.72 35.6 -22.2 ## 3 Greenwood A 2002 7.46 35.6 -19.2 ## 4 Greenwood A 2003 7.22 32.5 -28.8 ## 5 Kentville Cda Cs 2000 7.62 32.6 -21.9 ## 6 Kentville Cda Cs 2001 8.01 36.4 -20.3 ## 7 Kentville Cda Cs 2002 NA NA NA ## 8 Kentville Cda Cs 2003 7.32 31.4 -22.8 You will notice that in 2002 the mean temperature for Kentville appears to be NA, or missing. This is because R propogates missing values unless you explicitly tell it not to. To fix this, you could replace mean(mean_temp) with mean(mean_temp, na.rm = TRUE). Other useful functions to use inside summarise() include mean(), median(), sd(), sum(), min(), and max(). These all take a vector of values and produce a single aggregate value suitable for use in summarise(). One special function, n(), you can use (with no arguments) inside summarise() to tell you how many observations were aggregated to produce the values in that row. valley_climate %&gt;% group_by(station_name, year) %&gt;% summarise(mean_temp = mean(mean_temp, na.rm = TRUE), max_temp = max(extr_max_temp, na.rm = TRUE), min_temp = min(extr_min_temp, na.rm = TRUE), n = n()) ## # A tibble: 8 x 6 ## # Groups: station_name [?] ## station_name year mean_temp max_temp min_temp n ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Greenwood A 2000 7.54 31.8 -22.3 12 ## 2 Greenwood A 2001 7.72 35.6 -22.2 12 ## 3 Greenwood A 2002 7.46 35.6 -19.2 12 ## 4 Greenwood A 2003 7.22 32.5 -28.8 12 ## 5 Kentville Cda Cs 2000 7.62 32.6 -21.9 12 ## 6 Kentville Cda Cs 2001 8.01 36.4 -20.3 12 ## 7 Kentville Cda Cs 2002 6.68 33.5 -20.6 12 ## 8 Kentville Cda Cs 2003 7.32 31.4 -22.8 12 Unsurprisingly, there are twelve observations per year, since there are twelve rows (one per month) in our original data frame. It’s always a good idea to include n() inside summarise(), if nothing else as a check to make sure you’ve used group_by() with the correct columns. 2.9.1 Excercises Assign the data frame we just created to a variable, and inspect it using View() and str(). Which years were the warmest? Which years were the coldest? Create a similar data frame to the one we just created but with precipitation. In which years did the most precipitation fall? (Hint: you will need to use the sum() function) Most of the data are from Greenwood with fewer years from Kentville. How many years are available for each station, and what is the range of years where data is available for each station? (Hint: you will need to group_by(station_name)) 2.10 Extracting Columns When we use select(), we get back a data frame, however occasionally we need one or a few of the vectors that make up the data frame (recall from the last tutorial that data frames are a collection of column vectors). If we needed just the temperature values, we can use the $ operator to extract a column vector. valley_climate$mean_temp ## [1] -4.2 -2.6 1.6 6.4 10.0 16.4 18.8 19.2 14.6 9.9 4.6 -3.2 -5.9 -5.3 ## [15] -1.0 3.8 12.3 18.2 19.1 21.3 16.4 11.1 5.3 0.8 -3.0 -3.3 -0.1 5.3 ## [29] 11.2 14.7 19.3 20.2 NA 8.0 3.3 -2.1 -7.8 -7.2 -1.6 3.8 10.0 16.8 ## [43] 21.3 20.0 17.0 10.3 5.3 -0.1 -4.4 -3.1 2.0 6.4 10.4 16.2 19.0 18.8 ## [57] 14.1 9.2 5.0 -3.1 -6.5 -5.6 -0.8 4.2 12.3 18.4 19.2 20.6 15.5 10.7 ## [71] 4.5 0.2 -3.2 -2.9 0.2 5.2 11.1 14.7 19.7 20.1 16.1 7.6 3.4 -2.5 ## [85] -7.7 -7.3 -0.9 3.8 10.5 16.8 21.0 19.7 16.3 9.8 5.2 -0.5 The problem with doing this is that our mean temperature values no longer have any context: half of them are from one station and half of them are from another, but this is not reflected without the other columns! Nevertheless, many R functions outside of the tidyverse require input as vectors (including many you’ve used so far, including mean(), max(), min(), etc.), and you will often see the $ used in code written in other places to refer to columns. Functions in the tidyverse allow you to refer to columns by name (without the $) when used within specific functions (summarise() is a good example), so you should do this whenever you can! In the statistics tutorial we will start to use functions that are inconvenient to apply within the context of tidyverse and the pipe, but until then, get used to using summarise() and other tidyverse methods to call functions like mean(), max(), and min(). Later, we will introduce ways to calculate new columns using mutate(), which also let us refer to columns by name. 2.11 Base R Subsetting vs. select() and filter() In the wild, there are many ways to select columns and filter rows. I highly reccomend using filter() and select() to do this when writing new code, but you may see R code that subsets a data frame using square brackets in the form my_data_frame[c(&quot;column_name_1&quot;, &quot;column_name_2&quot;)] or my_data_frame[my_data_frame$column_name_1 &gt; some_number, c(&quot;column_name_1&quot;, &quot;column_name_2&quot;)]. The latter is equivalent to my_data_frame %&gt;% select(column_name_1, column_name_2) %&gt;% filter(column_name_1 &gt; some_number). The tidyverse method of subsetting I find to be much more clear and far less error-prone, but it’s worth knowing the other form so you can read R code written by others! 2.12 Summary In this tutorial we introduced the use of select(), filter(), arrange(), distinct(), and the pipe (%&gt;%). We also used group_by() and summarise() to provide summary statistics from a data frame. These functions are the building blocks of other powerful tools in the tidyverse. For more information, see the Data transformation chapter in R for Data Science. Another good resource is the tidyverse, visualization, and manipulation basics tutorial from Garrett Grolemund. "],
["creating-visualizations-using-ggplot.html", "Tutorial 3 Creating Visualizations using ggplot 3.1 Prerequisites 3.2 Using ggplot 3.3 Aesthetics 3.4 Geometries 3.5 Facets 3.6 Make it look pretty 3.7 Summary", " Tutorial 3 Creating Visualizations using ggplot Intro For more information, see the data visualization chapter in R for Data Science. 3.1 Prerequisites The prerequisite for this tutorial is the tidyverse package. If this package isn’t installed, you’ll have to install it using install.packages(). install.packages(&quot;tidyverse&quot;) Load the packages when you’re done! If there are errors, you may have not installed the above packages correctly! library(tidyverse) Finally, you will need to obtain the example data. Copy/paste the statement below to load the sample data for this tutorial (you can also download the dataset as a csv). valley_climate &lt;- read_csv(&quot;http://paleolimbot.github.io/r4wrs/data/valley_climate.csv&quot;, col_types = cols(.default = col_guess())) It’s worth mentioning a little bit about what this data frame contains, since we’ll be working with it for the rest of this tutorial. Each row contains a number of parameters that are available on a monthly basis from two Environment Canada climate stations (Kentville Agricultural Research Station and Greenwood Station, both located in the Annapolis Valley of Nova Scotia). The station_name, year, and month columns identify where and when the values were measured, and the rest of the columns contain the measured values. The only column names that are slightly cryptic are extr_max_temp and extr_min_temp, which are the extreme maximum and minimum temperatures measured in that month, respectively. 3.2 Using ggplot The Grammar of Graphics (the “gg” in “ggplot”) is a way of describing a graphic that is derived from data, which in R is done using the ggplot() function and its many friends. Unlike other plotting functions, ggplot() builds graphics from the data up (rather than starting with a template of a graphic and working backward). We will start with an example: ggplot(data = valley_climate, mapping = aes(x = date, y = total_precip, colour = station_name)) + geom_line() What the structure of the ggplot() call is Steps for plotting: Envision how you want your plot to look (draw it on paper if you have to!) Setup the data (select(), filter()) Setup your mapping (aes()) Choose your geoms (geom_*()) Make it look pretty 3.3 Aesthetics Categorical/Grouping Variables get mapped to X, Y, Colour, Shape, Linetype Continuous Variables get mapped to X, Y, Colour, Size 3.4 Geometries Different geometry… ggplot(data = valley_climate, mapping = aes(x = date, y = total_precip, colour = station_name)) + geom_point() ## Warning: Removed 2 rows containing missing values (geom_point). Multiple geometries! ggplot(data = valley_climate, mapping = aes(x = date, y = total_precip, colour = station_name)) + geom_point() + geom_line() ## Warning: Removed 2 rows containing missing values (geom_point). 3.5 Facets A way to make multiple groups 3.6 Make it look pretty 3.6.1 Labels labs() function 3.6.2 Scales scale_*_discrete(), scale_*_continuous() 3.7 Summary Tutorial summary For more information, see the data visualization chapter in R for Data Science. "],
["preparing-and-loading-your-data-into-r.html", "Tutorial 4 Preparing and Loading your data into R 4.1 Prerequisites 4.2 Preparing data 4.3 Loading data 4.4 Cleaning data 4.5 Summary", " Tutorial 4 Preparing and Loading your data into R Intro Draws from data transformation and data import from R for Data Science. 4.1 Prerequisites The prerequisites for this tutorial are tidyverse. If this package isn’t installed, you’ll have to install it using install.packages(). install.packages(&quot;tidyverse&quot;) Load the package when you’re done! If there are errors, you may have not installed the above packages correctly! library(tidyverse) Finally, you will need to obtain the sample data. We will be using the same sample data as before (valley_climate), but instead you will be using data files that contain this data in various forms. You can download this data in comma-separated format (CSV), tab-separated format (TSV), and Microsoft Excel format, as well as the raw files (like you would download from Environment Canada) for Kentville and Greenwood. You’ll need all six files to do the excercises in this tutorial. 4.2 Preparing data Data comes in a nearly infinite variety of formats and configurations. Occasionally, you are in charge of entering data yourself, in which case you can plan ahead on how your data looks. 4.3 Loading data 4.4 Cleaning data 4.5 Summary Tutorial summary Draws from data transformation and data import from R for Data Science. "],
["using-rstudio-projects.html", "Tutorial 5 Using RStudio Projects 5.1 Prerequisites 5.2 Projects 5.3 Creating a new project 5.4 Organizing a project 5.5 Why use a project? 5.6 Summary", " Tutorial 5 Using RStudio Projects Even if you know how to code in R, you will still need a strategy of how to organize your data, code, and output to take advantage of all the things R and RStudio have to offer. This tutorial suggests one way of doing this, with the take home message that you should stay organized! 5.1 Prerequisites The only prerequisite for this tutorial is RStudio. This tutorial draws heavily on the Projects chapter and the Scripts chapter in R for Data Science by Hadley Wickham. 5.2 Projects Projects are a notion in RStudio that let you keep a set of related data and scripts organized in a single directory. R scripts are rarely useful without data files, and data files are rarely useful on their own when using R for data analyis. Furthermore, the figures and tables you generate using R are only replicable if you know which script created them! An RStudio project lets you keep your data, scripts, and output in the same working directory, so that it is easy to refer to your data from your files, and easy to know which scripts generated your analysis. 5.3 Creating a new project To create a new project, chose New project… from the File menu, or click on the little RStudio icon in the upper right side of your screen to open the New project dialog. The project menu in RStudio is located in the upper right corner of the screen. When you have created more projects, you can easily switch between them here. This will direct you to a dialog that gives you some options about how to create your RStudio project. If you already have a folder that contains your scripts and/or data, you can use the Existing Directory option to turn that directory into a project. To create a brand new directory that is an RStudio project, choose the New Directory option. This will give you another menu, from which you’ll want to choose Empty Project. This will give you a window with a few options about where to create your project. Creating an RStudio project requires a directory name and a place where that directory lives. Name the project something descriptive and put it somwhere you can find it! The Desktop is not a bad place to start (you can move projects around after you’ve created them). 5.4 Organizing a project Most projects have at least three components: data, scripts, and output (once you get to using R Markdown you might have a fourth, documents). A well-organized project has defined places where these things live. I tend to put raw data (data that I have never touched, which usually means it is from an instrument or the internet or from somebody else’s spreadsheet) in a folder called raw_data/, and cleaned data in a folder called data/. Scripts (.R files) can live in a folder called scripts/, and output can live in a folder called output/ or figures/. R Markdown files (documents) need to live in the main project directory for a very good reason that you will learn when we get to using R Markdown. The point is not that you need to or should have specific folders, but that it is clear to you and others where to look for the various components of your project. A directory structure for a recent project collecting Halifax Water DOC measurements. The raw data folder contains an excel sheet from Halifax Water, and the clean_data.R script reads the excel sheet and generates halifax_wq.csv, which is read by all the scripts that do analysis and generate figures within the project. 5.5 Why use a project? The main advantage of using a project is that the working directory when you open one is automatically set to the directory of the project. This means that you never have to call setwd(), since your data always lives fairly close to your R script. In the above example of a recent RStudio project, hw_toc_plot.R reads in halifax_wq.csv using the command read_csv(&quot;data/halifax_wq.csv&quot;). No matter where the project lives on the computer (or indeed, whose computer the project lives on), this script can remain unchanged. This gives you the freedom to move the project folder about your computer, or to have it live somewhere like Dropbox or GitHub such that the project can be shared with others. 5.6 Summary This tutorial suggested one method of organizing R scripts and data to leverage the full potential of R and RStudio. Projects are an easy way to collect data, scripts, and output together in the same folder so that output can be replicated and analysis in scripts can be easily regenerated by yourself and others. "],
["computing-statistics-in-r.html", "Tutorial 6 Computing statistics in R 6.1 Prerequisites 6.2 Terminology 6.3 Testing for Normality 6.4 Correlation and Linear Regression 6.5 Significant differences 6.6 Summary", " Tutorial 6 Computing statistics in R Intro 6.1 Prerequisites The prerequisites for this tutorial are the tidyverse and broom packages. If these packages aren’t installed, you’ll have to install them using install.packages(). install.packages(&quot;tidyverse&quot;) # will also install the broom package Load the packages when you’re done! If there are errors, you may have not installed the above packages correctly! library(tidyverse) library(rclimateca) library(broom) Finally, you will need to obtain the sample data… climate_data &lt;- getClimateData(c(27141, 6354), year = 2000:2003, nicenames = TRUE) %&gt;% left_join(ecclimatelocs %&gt;% select(station_name = Name, stationid = `Station ID`), by = &quot;stationid&quot;) %&gt;% select(station_name, year, month, everything()) %&gt;% select(-ends_with(&quot;flag&quot;), -parseddate, -datetime, -stationid) ## getClimateData() is deprecated and will be removed in future versions: use ec_climate_data() instead ## getClimateDataRaw() is deprecated and will be removed in future versions: use ec_climate_data_base() instead ## getClimateDataRaw() is deprecated and will be removed in future versions: use ec_climate_data_base() instead 6.2 Terminology 6.2.1 Grouping Variables Many statistical test functions in R use a formula to specify a value column and a grouping column for a test using a data frame as input. This is usually a column that contains data labels like “group1”, “group1”, “group1”, “group2”, “group2”, etc., whose values divide observations into groups for which we want to test significance. In our sample data frame climate_data, these variables are station_name, year, month. The other variables represent measured values, whereas the grouping variables give context to each row. The formula used as input to statistical test functions is generally in the form measure_var ~ grouping_var, where measure_var and grouping_var are columns in a data frame. Sometimes it is necessary to use column names as grouping variables. In the case of the climate_data data frame, one might want to test whether the mean monthly temperature (meantemp) is significantly different than the extreme maximum monthly temperature (extrmaxtemp). Because these values are stored in two columns, there is no grouping variable that can separate these two sets of observations. This operation is possible using the gather() function. climate_data_mean_max_temp &lt;- climate_data %&gt;% select(station_name, year, month, meantemp, extrmaxtemp) %&gt;% gather(meantemp, extrmaxtemp, key = &quot;temperature_type&quot;, value = &quot;temp&quot;) climate_data_mean_max_temp ## # A tibble: 192 x 5 ## station_name year month temperature_type temp ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 KENTVILLE CDA CS 2000 1 meantemp - 4.20 ## 2 KENTVILLE CDA CS 2000 2 meantemp - 2.60 ## 3 KENTVILLE CDA CS 2000 3 meantemp 1.60 ## 4 KENTVILLE CDA CS 2000 4 meantemp 6.40 ## 5 KENTVILLE CDA CS 2000 5 meantemp 10.0 ## 6 KENTVILLE CDA CS 2000 6 meantemp 16.4 ## 7 KENTVILLE CDA CS 2000 7 meantemp 18.8 ## 8 KENTVILLE CDA CS 2000 8 meantemp 19.2 ## 9 KENTVILLE CDA CS 2000 9 meantemp 14.6 ## 10 KENTVILLE CDA CS 2000 10 meantemp 9.90 ## # ... with 182 more rows In the resulting data frame, the temperature_type column will contain the values “meantemp” and “extrmaxtemp”, which can be used as a grouping variable in a statistical test function. When using gather(), it is important to select only the relevant variables using select() first. 6.2.2 Paired Values Some statistical tests are only possible or are preferable with paired values, or values that are stored in two columns in a data frame (i.e., part of the same observation). The example above of testing whether the mean monthly temperature (meantemp) is significantly different than the extreme maximum monthly temperature (extrmaxtemp) could also be done pairwise, in which case the original data is already in the correct format for the test. Sometimes the data is provided in a form where there is a grouping variable, and measured values are in the same column (this is the result of the above example where we converted meantemp and extrmaxtemp into a grouping variable and a measured variable). We can transform a grouping variable and a measured variable into paired observations using the spread() function. climate_data_mean_max_temp %&gt;% spread(key = temperature_type, value = temp) ## # A tibble: 96 x 5 ## station_name year month extrmaxtemp meantemp ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GREENWOOD A 2000 1 18.0 - 4.40 ## 2 GREENWOOD A 2000 2 17.8 - 3.10 ## 3 GREENWOOD A 2000 3 17.9 2.00 ## 4 GREENWOOD A 2000 4 20.0 6.40 ## 5 GREENWOOD A 2000 5 21.3 10.4 ## 6 GREENWOOD A 2000 6 31.8 16.2 ## 7 GREENWOOD A 2000 7 29.4 19.0 ## 8 GREENWOOD A 2000 8 29.0 18.8 ## 9 GREENWOOD A 2000 9 31.0 14.1 ## 10 GREENWOOD A 2000 10 21.9 9.20 ## # ... with 86 more rows In the resulting data frame, the values that were in temperature_type are now column names, and the qualifying variables station_name, year, and month are used to identify unique observations that are paired with one another. 6.2.3 Independent Observations …don’t have a good explanation for this. 6.2.4 Graphical Test autocorrelation? 6.3 Testing for Normality Some tests (notably the t-test, the ANOVA test, and the Pearson coefficient) require that the input values are normally distributed. For small amounts of replicate samples, this is generally a good assumption, however larger samples whose distribution cannot be assumed require a test for normality. One such test is the Shapiro-Wilk test, which is described below. 6.3.1 Test Data The test for normality requires a data frame with one column that contains the values that should be normally distributed. In our example, we will test whether or not mean monthly temperature is normally distributed, and whether or not total montly precipitation is normally distributed. It is also good practice to keep qualifying variables that give context to each observation. normal_test_data &lt;- climate_data %&gt;% select(station_name, year, month, meantemp, totalprecip) normal_test_data ## # A tibble: 96 x 5 ## station_name year month meantemp totalprecip ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 KENTVILLE CDA CS 2000 1 - 4.20 182 ## 2 KENTVILLE CDA CS 2000 2 - 2.60 10.7 ## 3 KENTVILLE CDA CS 2000 3 1.60 116 ## 4 KENTVILLE CDA CS 2000 4 6.40 89.1 ## 5 KENTVILLE CDA CS 2000 5 10.0 58.0 ## 6 KENTVILLE CDA CS 2000 6 16.4 46.0 ## 7 KENTVILLE CDA CS 2000 7 18.8 72.4 ## 8 KENTVILLE CDA CS 2000 8 19.2 36.6 ## 9 KENTVILLE CDA CS 2000 9 14.6 58.4 ## 10 KENTVILLE CDA CS 2000 10 9.90 198 ## # ... with 86 more rows 6.3.2 Graphical Test The graphical test for a normality test is a histogram. ggplot(normal_test_data, aes(x = meantemp)) + geom_histogram(bins = 30) ## Warning: Removed 1 rows containing non-finite values (stat_bin). A historgram of a normally distributed variable should be symmetrical about its mean, like the histogram shown below: set.seed(300) normal_random_data &lt;- tibble(normal_random_data = rnorm(n = 100, mean = 0, sd = 1)) ggplot(normal_random_data, aes(x = normal_random_data)) + geom_histogram(bins = 30) A histogram of a normally distributed variable generally isn’t a perfect bell curve (especially when there are few data points), but should show some evidence of symmetry about the mean value. 6.3.3 Statistical Test Testing for normality in R involves a call to shapiro.test(), followed by a call to tidy() in the broom package to view the results in the form of a data frame. shapiro.test(normal_test_data$meantemp) %&gt;% tidy() ## # A tibble: 1 x 3 ## statistic p.value method ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.936 0.000163 Shapiro-Wilk normality test A low p.value indicates that there is evidence to reject the notion that the input data are sampled from a normally distributed population. You will have to pick a level of significance (\\(\\alpha\\)) as a threshold, usually 0.05 or 0.01, under which the p-value will indicate the sample was drawn from a non-normal distribution. In our example, the mean montly temperature was shown to be non-normally distributed (p&lt;0.001). In the case of our normal random data, the p-value is quite high (p=0.72), suggesting that the data were sampled from a normally distributed population. shapiro.test(normal_random_data$normal_random_data) %&gt;% tidy() ## # A tibble: 1 x 3 ## statistic p.value method ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.991 0.722 Shapiro-Wilk normality test 6.4 Correlation and Linear Regression Tests for correlation of two variables test whether or not a relationship exists between the two variables (i.e., can any of the variance of one variable be explained by variance in the other). This is often done to test association between two parameters when these measurements are paired. In our example, we will test the corellation between mean monthly temperature and total monthly precipitation, to ascertain whether or not a statistically significant relationship exists between the two. 6.4.1 Test Data Correlation tests all require a data frame with one column for the x variable and one column for the y variable. It is often useful to keep other qualifying variables that give context to each observation, but are not required by the test. In our case, the x variable will be meantemp and the y variable will be totalprecip. correlation_test_data &lt;- climate_data %&gt;% select(station_name, year, month, meantemp, totalprecip) correlation_test_data ## # A tibble: 96 x 5 ## station_name year month meantemp totalprecip ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 KENTVILLE CDA CS 2000 1 - 4.20 182 ## 2 KENTVILLE CDA CS 2000 2 - 2.60 10.7 ## 3 KENTVILLE CDA CS 2000 3 1.60 116 ## 4 KENTVILLE CDA CS 2000 4 6.40 89.1 ## 5 KENTVILLE CDA CS 2000 5 10.0 58.0 ## 6 KENTVILLE CDA CS 2000 6 16.4 46.0 ## 7 KENTVILLE CDA CS 2000 7 18.8 72.4 ## 8 KENTVILLE CDA CS 2000 8 19.2 36.6 ## 9 KENTVILLE CDA CS 2000 9 14.6 58.4 ## 10 KENTVILLE CDA CS 2000 10 9.90 198 ## # ... with 86 more rows 6.4.2 Graphical Test A graphical test of the correlation of two variables is a biplot with one variable on the x-axis, and one variable on the y-axis. The variable on the x-axis should be the indepenent variable for the purposes of the test. This will look something like ggplot(my_data_frame, x = independent_var, y = dependent_var) followed by geom_point(). You can add a linear regression to the plot using stat_smooth(method = lm). This will add the best-fit line whose slope and intercept we will calculate in the next section. ggplot(correlation_test_data, aes(x = meantemp, y = totalprecip)) + geom_point() + stat_smooth(method = lm) ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). Based on inspection of the biplot, you should be able to have a hunch as to whether or not a linear relationship exists between the two variables. In our case, it looks like there is a weak negative correlation between mean temperature and total preciptiation (i.e., the higher the mean temperature for a given month, the lower the total precipitation for the same month). 6.4.3 The Pearson product-moment correlation coefficient (r) The Pearson product-moment correlation coefficient (usually known as the r value) is a test of how well a line of best fit is able to model the data (generally a standard least-squares linear regression). The coefficient ranges from -1 (perfect negative linear relationship) to +1 (perfect positive linear relationship). Generally the square of this value is reported (r2), and can be interpreted as “xx % of the variance in y_variable can be explained by the variance in x_variable”. There is no statistical way to test how good the linear relationship is, but it is possible to test that the coefficient is not equal to zero (i.e., it is possible to reject the notion that there x_variable and y_variable have no linear relationship). 6.4.3.1 Assumptions The Pearson product-moment correlation coefficient assumes that x_variable and y_variable are normally distributed. 6.4.3.2 Statistical Test Calculating the r value and associated p-value involves a call to cor.test() with method = &quot;pearson&quot;, followed by a call to tidy() in the broom package to get the test results in the form of a data frame. cor.test(~totalprecip + meantemp, data = correlation_test_data, method = &quot;pearson&quot;) %&gt;% tidy() ## # A tibble: 1 x 8 ## estimate statistic p.value parameter conf.low conf.high method ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.304 -3.07 0.00286 92 -0.478 -0.108 Pearson&#39;s produ… ## # ... with 1 more variable: alternative &lt;fct&gt; The estimate column contains the r value, which you could square to get the r2 value. The p.value column contains the p-value, which represents the probability that the two variables have no linear relationship. In our case, totalprecip and meantemp have a significant negative linear relationship (p=0.003). 6.4.4 Spearman \\(\\rho\\) or rs The Spearman correlation coefficient (abbreviated \\(\\rho\\) or rs) is a test of a one-to-one relationship between x_variable and y_variable, not necessarily linear. The test uses ranked values for x_variable and y_variable, so outliers are less of an issue than they are with the Pearson coefficient. Similar to the Perason coefficient, the rs value varies from -1 (a perfect one-to-one negative relationship) to 1 (a perfect one-to-one positive relationship). Similar to the Pearson coefficient, it is only possible to test that the value is not equal to zero (i.e., i.e., it is possible to reject the notion that there x_variable and y_variable have no one-to-one relationship). 6.4.4.1 Assumptions The Spearman correlation coefficient does not make any assumptions about the distribution of x_variable or y_variable. 6.4.4.2 Statistical Test Calculating the rs value and associated p-value involves a call to cor.test() with method = &quot;spearman&quot;, followed by a call to tidy() in the broom package to get the test results in the form of a data frame. cor.test(~totalprecip + meantemp, data = correlation_test_data, method = &quot;spearman&quot;) %&gt;% tidy() ## Warning in cor.test.default(x = c(182.2, 10.7, 116.4, 89.1, 58, 46, 72.4, : ## Cannot compute exact p-value with ties ## # A tibble: 1 x 5 ## estimate statistic p.value method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 -0.381 191102 0.000154 Spearman&#39;s rank correlation rho two.sided The estimate column contains the rs value, and the p.value column contains the p-value, which represents the probability that the two variables are not correlated. In our case, totalprecip and meantemp have a significant negative relationship (p&lt;0.001). 6.4.5 Linear Regression Whereas a Pearson coefficient is meant to assess the quality of a linear relationship, linear regression is meant to determine the slope and intercept of that relationship in the form \\(y = mx + b\\), where \\(y\\) is y_variable, and x is x_variable. By obtaining \\(m\\) and \\(b\\), we can use x_variable to calculate y_variable for any value of x_variable. 6.4.5.1 Assumptions The standard linear regression (a least-squares regression) works best if both x_variable and y_variable are symmetrically distributed. 6.4.5.2 Statistical Test Calculating the coefficients \\(m\\) and \\(b\\) for a linear regression involves a call to lm() with a formula y_variable ~ x_variable (note this is slightly different than for correlation testing) and data = my_data_frame. For our example, the call would look like this: lm(totalprecip ~ meantemp, data = correlation_test_data) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 101 5.45 18.5 8.13e⁻³³ ## 2 meantemp - 1.44 0.469 - 3.07 2.86e⁻ ³ The term column in the ouput refers to the name of the input column in the righthand side of the input formula, or “(Intercept)” for the intercept, and the estimate column refers to the coefficient itself. In the example, this means we can predict totalprecip using the (approximate) expression -1.44 * meantemp + 101.08. In practice, we want to use the predict() function to do this math for us (because if we change some code above that alters which observations are used to create the regression, it will change the coefficient and intercept, and any code that relies on the hard-coded version will be incorrect). This is a three step process: first, save the result of lm() to a variable, then create a data frame with a column that has the same name as x_variable, then use mutate() to create a new column with the predictions from predict(). Note that we use a special trick in mutate() to pass the entire data frame to the newdata argument of predict() (the . represents the whole data frame as opposed to any particular column, which we can refer to by name within mutate()). For our example, we might be interested in the predicted total monthly precipitation values when the mean monthly temperature is 5, 10, 15, and 20 degrees. model &lt;- lm(totalprecip ~ meantemp, data = climate_data) tibble(meantemp = c(5, 10, 15, 20)) %&gt;% mutate(totalprecip_predicted = predict(model, newdata = .)) ## # A tibble: 4 x 2 ## meantemp totalprecip_predicted ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5.00 93.9 ## 2 10.0 86.7 ## 3 15.0 79.5 ## 4 20.0 72.3 6.5 Significant differences Tests for significant differences tests whether or not there is a significant difference among various groups of observations. Which test to use depends on whether or not the data are normally distributed, and how many groups exist. For our example, we will be looking at the diferences in mean temperature (meantemp) as grouped by several grouping variables (station_name, year, and month). 6.5.1 Test Data Tests for significant differences require a data frame with a column containing the values to test, and a column containing the variable to group by (usually contains strings like “group1”, “group2”, “group3”, etc.). It is often useful to keep other qualifying variables that give context to each observation, but are not required by the test. In our case, the column in climate_data that contains the values we are testing is meantemp, and the columns that contain the groups are station_name, year, and month. difference_test_data &lt;- climate_data %&gt;% select(station_name, year, month, meantemp) difference_test_data ## # A tibble: 96 x 4 ## station_name year month meantemp ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 KENTVILLE CDA CS 2000 1 - 4.20 ## 2 KENTVILLE CDA CS 2000 2 - 2.60 ## 3 KENTVILLE CDA CS 2000 3 1.60 ## 4 KENTVILLE CDA CS 2000 4 6.40 ## 5 KENTVILLE CDA CS 2000 5 10.0 ## 6 KENTVILLE CDA CS 2000 6 16.4 ## 7 KENTVILLE CDA CS 2000 7 18.8 ## 8 KENTVILLE CDA CS 2000 8 19.2 ## 9 KENTVILLE CDA CS 2000 9 14.6 ## 10 KENTVILLE CDA CS 2000 10 9.90 ## # ... with 86 more rows 6.5.2 Graphical Test The graphic for significant difference tests is a plot with the grouping variable on the x-axis, and the value variable on the y-axis. This is generated using something like ggplot(my_data_frame, aes(x = group_column, y = value_column)) followed by geom_point() and/or geom_boxplot(). If the grouping variable is station_name, such a plot might look like this: ggplot(difference_test_data, aes(x = station_name, y = meantemp)) + geom_boxplot() ## Warning: Removed 1 rows containing non-finite values (stat_boxplot). For smaller numbers of observations, it may make sense to plot the values of the observations themselves using geom_point(). In the next example, the grouping variable is month (note that we have to use factor(month) in ggplot, because we are using a continuous variable as a grouping variable). ggplot(difference_test_data, aes(x = factor(month), y = meantemp)) + geom_point() ## Warning: Removed 1 rows containing missing values (geom_point). When there are a small number of observations in each group, it also may make sense to compute summary statistics and plot those instead of a boxplot or the observations themselves. This is done using stat_summary(), which by default displays a point with error bars plus or minus the standard error (the standard deviation divided by the square root of n). ggplot(difference_test_data, aes(x = factor(month), y = meantemp)) + stat_summary(size = 0.25) ## Warning: Removed 1 rows containing non-finite values (stat_summary). ## No summary function supplied, defaulting to `mean_se() Based on the graphic, you should be able to have a hunch as to whether or not one group of observations is significantly different than another group of observations (when grouped by station, it looks like there isn’t much difference in temperature, but when grouped by month, there is a clear difference). This is important, because it will make interpreting your results more intuitive and allows you to check for errors. 6.5.3 The t-test The t-test tests whether or not there is a significant difference between a value exactly two groups of observations (an ANOVA test can be used when there are more than two groups). We will be using this test to ascertain whether or not there is a significant difference in temperature when these observations are grouped by station (note that there are exactly two stations, Kentville and Greenwood). 6.5.3.1 Assumptions The t-test assumes that the two samples of data values are normally distributed and independent. 6.5.3.2 Statistical Test Performing the t-test uses a call to the t.test() function in the form t.test(value_column ~ group_column, data = my_data_frame), and a call to the tidy() function in the broom package to view the results in the form of a data frame. In the case of the Kentville/Greenwood climate data, the two tests look like this: t.test(meantemp ~ station_name, data = difference_test_data) %&gt;% tidy() ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0641 7.49 7.42 0.0347 0.972 92.9 -3.60 ## # ... with 3 more variables: conf.high &lt;dbl&gt;, method &lt;fct&gt;, ## # alternative &lt;fct&gt; Here the estimate column is the estimated difference between the means of the two groups, and the p.value column is the p-value, which represents the probability that there is no significant difference between the two groups (p=0.97). 6.5.4 Paired t-test Paired version of the t-test… 6.5.5 Wilcox Rank Sum/Mann-Whitney Test The Wilcox Rank Sum Test is… 6.5.6 The ANOVA test The ANOVA test tests whether or not there is a significant difference between a value using two or more groups of observations (an ANOVA test when there are only two groups is identical to a t-test). We will be using this test to ascertain whether or not there is a significant difference in mean monthly temperature when these observations are grouped by (1) year and (2) month. 6.5.6.1 Assumptions The ANOVA test assumes all samples of data values are normally distributed and independent. 6.5.6.2 Statistical Test Performing the ANOVA test uses a call to the aov() function in the form aov(value_column ~ group_column, data = my_data_frame), and a call to the tidy() function in the broom package to view the results in the form of a data frame. In the case of the Kentville/Greenwood climate data, the two tests look like this: aov(meantemp ~ year, data = difference_test_data) %&gt;% tidy() ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 year 1.00 3.48 3.48 0.0430 0.836 ## 2 Residuals 93.0 7533 81.0 NA NA aov(meantemp ~ month, data = difference_test_data) %&gt;% tidy() ## # A tibble: 2 x 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 month 1.00 1058 1058 15.2 0.000184 ## 2 Residuals 93.0 6478 69.7 NA NA Generally, the only column we care about in the output is the p.value, which is the probability that none of the groups of values are significantly different than any others. In the case of the Kentville/Greenwood climate data, there is a significant difference in temperature among months (p&lt;0.001), but no significant different in temperature among years (p=0.83). 6.5.7 Krustal-Wallis Rank Sum Test Krustal Wallis Test… 6.6 Summary Tutorial summary "]
]
